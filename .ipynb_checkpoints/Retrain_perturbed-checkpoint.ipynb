{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sign = lambda a: (a>0) - (a<0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(x_new1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new1 = np.load('/tmp/x_new1.npy')\n",
    "x_new5 = np.load('/tmp/x_new5.npy')\n",
    "x_new10 = np.load('/tmp/x_new10.npy')\n",
    "x_new20 = np.load('/tmp/x_new20.npy')\n",
    "x_new30 = np.load('/tmp/x_new30.npy')\n",
    "x_new40 = np.load('/tmp/x_new40.npy')\n",
    "x_new50 = np.load('/tmp/x_new50.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 20\n",
    "batch_size = 100\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition => 10 classes\n",
    "\n",
    "# Set model weights\n",
    "W1 = tf.Variable(tf.random_normal([784, 300], mean=0, stddev=1))\n",
    "b1 = tf.Variable(tf.random_normal([300], mean=0, stddev = 1))\n",
    "\n",
    "#W2 = tf.Variable(tf.random_normal([300, 100], mean=0, stddev= 1))\n",
    "#b2 = tf.Variable(tf.random_normal([100], mean=0, stddev= 1))\n",
    "\n",
    "W3 = tf.Variable(tf.zeros([300, 10]))\n",
    "b3 = tf.Variable(tf.zeros([10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct model\n",
    "\n",
    "hidden1 = tf.nn.relu(tf.matmul(x, W1) + b1); #first hidden layer\n",
    "\n",
    "#hidden2 = tf.nn.relu(tf.matmul(hidden1, W2) + b2); #second hidden layer\n",
    "\n",
    "pred = tf.nn.softmax(tf.matmul(hidden1, W3) + b3) # Softmax layer outputs prediction probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Minimize error using cross entropy \n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimizer = {}\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# for name, fcn in cost:\n",
    "#     optimizer[\"optimizer_{0}\".format(name)] = tf.train.GradientDescentOptimizer(learning_rate).minimize(fcn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_comb1 = np.append(mnist.train.images, x_new1[:3000], axis = 0)\n",
    "x_comb5 = np.append(mnist.train.images, x_new5[:3000], axis = 0)\n",
    "x_comb10 = np.append(mnist.train.images, x_new10[:3000], axis = 0)\n",
    "x_comb20 = np.append(mnist.train.images, x_new20[:3000], axis = 0)\n",
    "x_comb30 = np.append(mnist.train.images, x_new30[:3000], axis = 0)\n",
    "x_comb40 = np.append(mnist.train.images, x_new40[:3000], axis = 0)\n",
    "x_comb50 = np.append(mnist.train.images, x_new50[:3000], axis = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(mnist.train.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58000, 784)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_comb1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_comb = np.append(mnist.train.labels, mnist.test.labels[:3000], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "# save_dir = 'checkpoints/'\n",
    "# if not os.path.exists(save_dir):\n",
    "#     os.makedirs(save_dir)\n",
    "# save_path = os.path.join(save_dir, 'trained_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.553866937\n",
      "Epoch: 0002 cost= 0.369853101\n",
      "Epoch: 0003 cost= 0.330218216\n",
      "Epoch: 0004 cost= 0.311045950\n",
      "Epoch: 0005 cost= 0.296482438\n",
      "Epoch: 0006 cost= 0.289432846\n",
      "Epoch: 0007 cost= 0.281418635\n",
      "Epoch: 0008 cost= 0.274472754\n",
      "Epoch: 0009 cost= 0.271532540\n",
      "Epoch: 0010 cost= 0.265823214\n",
      "Epoch: 0011 cost= 0.262205086\n",
      "Epoch: 0012 cost= 0.260892021\n",
      "Epoch: 0013 cost= 0.254971765\n",
      "Epoch: 0014 cost= 0.253346635\n",
      "Epoch: 0015 cost= 0.250885308\n",
      "Epoch: 0016 cost= 0.244573751\n",
      "Epoch: 0017 cost= 0.245417036\n",
      "Epoch: 0018 cost= 0.244592970\n",
      "Epoch: 0019 cost= 0.241745012\n",
      "Epoch: 0020 cost= 0.240874855\n",
      "Optimization Finished!\n",
      "Accuracy: 0.918\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "\n",
    "## change epoch to 50\n",
    "# training_epochs = 20\n",
    "# display_step = 1\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(np.shape(x_comb1)[0]/batch_size)\n",
    "#         print(\"number of total_batch: %\"%s)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            batch_xs, batch_ys = next_batch(total_batch, x_comb1, y_comb)\n",
    "#             batch_xs, batch_ys = next_batch(total_batch, x_comb5, y_comb)\n",
    "#             batch_xs, batch_ys = next_batch(total_batch, x_comb10, y_comb)\n",
    "#             batch_xs, batch_ys = next_batch(total_batch, x_comb20, y_comb)\n",
    "#             batch_xs, batch_ys = next_batch(total_batch, x_comb30, y_comb)\n",
    "#             batch_xs, batch_ys = next_batch(total_batch, x_comb40, y_comb)\n",
    "#             batch_xs, batch_ys = next_batch(total_batch, x_comb50, y_comb)\n",
    "            \n",
    "            # Fit training using batch data\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,y: batch_ys})\n",
    "            \n",
    "#             print(__w)\n",
    "            \n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "#             print(sess.run(W))\n",
    "            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "#     # Training cycle\n",
    "#     for epoch in range(training_epochs):\n",
    "#         avg_cost = 0.\n",
    "#         total_batch = int(mnist.train.num_examples/batch_size)\n",
    "# #         print(\"number of total_batch: %\"%s)\n",
    "#         # Loop over all batches\n",
    "#         for i in range(total_batch):\n",
    "\n",
    "#         # Fit training using batch data\n",
    "#         _, c = sess.run([optimizer, cost], feed_dict={x: x_comb1,y: y_comb})\n",
    "            \n",
    "# #             print(__w)\n",
    "            \n",
    "#             # Compute average loss\n",
    "# #             avg_cost += c / total_batch\n",
    "#         # Display logs per epoch step\n",
    "#         if (epoch+1) % display_step == 0:\n",
    "# #             print(sess.run(W))\n",
    "#             print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c))\n",
    "\n",
    "    print (\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    \n",
    "    # Calculate accuracy for 3000 examples; you should get roughly ~90% accuracy although it might vary from run to run\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print (\"Accuracy:\", accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]}))\n",
    "    \n",
    "    \n",
    "    #save weights of the pertubed model\n",
    "#     saver.save(sess, '/tmp/trained_model1.ckpt')\n",
    "#     saver.save(sess, '/tmp/trained_model5.ckpt')\n",
    "#     saver.save(sess, '/tmp/trained_model10.ckpt')\n",
    "#     saver.save(sess, '/tmp/trained_model20.ckpt')\n",
    "#     saver.save(sess, '/tmp/trained_model30.ckpt')\n",
    "#     saver.save(sess, '/tmp/trained_model40.ckpt')\n",
    "#     saver.save(sess, '/tmp/trained_model50.ckpt')\n",
    "\n",
    "    print(\"Model saved\")\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model(no retraining).\n",
      "INFO:tensorflow:Restoring parameters from /tmp/trained_model.ckpt\n",
      "Accuracy of original train dataset: 0.929527\n",
      "Accuracy of original test dataset: 0.905667\n",
      "Accuracy of perturbed test dataset: 0.4664\n",
      "Accuracy of mixed perturbed FGSM dataset: 0.858277\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    print(\"Original model(no retraining).\")\n",
    "    saver.restore(sess, \"/tmp/trained_model.ckpt\")\n",
    "    \n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    \n",
    "    # Calculate accuracy for 3000 examples; you should get roughly ~90% accuracy although it might vary from run to run\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print (\"Accuracy of original train dataset:\", accuracy.eval({x: mnist.train.images, y: mnist.train.labels}))\n",
    "    print (\"Accuracy of original test dataset:\", accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]}))\n",
    "    print (\"Accuracy of perturbed test dataset:\", accuracy.eval({x: x_new50, y: mnist.test.labels}))\n",
    "    print (\"Accuracy of mixed perturbed FGSM dataset:\", accuracy.eval({x: np.append(x_new50, mnist.train.images, axis = 0), \n",
    "                                       y: np.append(mnist.test.labels, mnist.train.labels, axis = 0)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    print(\"Retrained model with eps = 1.\")\n",
    "    saver.restore(sess, \"/tmp/trained_model1.ckpt\")\n",
    "    \n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    \n",
    "    # Calculate accuracy for 3000 examples; you should get roughly ~90% accuracy although it might vary from run to run\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print (\"Accuracy of original train dataset:\", accuracy.eval({x: mnist.train.images, y: mnist.train.labels}))\n",
    "    print (\"Accuracy of original test dataset:\", accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]}))\n",
    "    print (\"Accuracy of perturbed(eps = 1) test dataset:\", accuracy.eval({x: x_new50, y: mnist.test.labels}))\n",
    "    print (\"Accuracy of mixed perturbed FGSM dataset:\", accuracy.eval({x: np.append(x_new50, mnist.train.images, axis = 0), \n",
    "                                       y: np.append(mnist.test.labels, mnist.train.labels, axis = 0)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/trained_model50.ckpt\n",
      "Model restored.\n",
      "Accuracy of original train dataset: 0.927891\n",
      "Accuracy of original test dataset: 0.903667\n",
      "Accuracy of perturbed test dataset: 0.9191\n",
      "Accuracy of mixed perturbed FGSM dataset: 0.926538\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    saver.restore(sess, \"/tmp/trained_model50.ckpt\")\n",
    "    print(\"Model restored for eps = 50.\")\n",
    "    \n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    \n",
    "    # Calculate accuracy for 3000 examples; you should get roughly ~90% accuracy although it might vary from run to run\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print (\"Accuracy of original train dataset:\", accuracy.eval({x: mnist.train.images, y: mnist.train.labels}))\n",
    "    print (\"Accuracy of original test dataset:\", accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]}))\n",
    "    print (\"Accuracy of perturbed test dataset:\", accuracy.eval({x: x_new50, y: mnist.test.labels}))\n",
    "    print (\"Accuracy of mixed perturbed FGSM dataset:\", accuracy.eval({x: np.append(x_new50, mnist.train.images, axis = 0), \n",
    "                                       y: np.append(mnist.test.labels, mnist.train.labels, axis = 0)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # saver = tf.train.Saver()\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#   # Restore variables from disk.\n",
    "#   saver.restore(sess, \"/tmp/model.ckpt\")\n",
    "#   print(\"Model restored.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yt = mnist.test.labels\n",
    "xt = mnist.test.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_x = tf.gradients(xs=xt, ys=cost)\n",
    "x_prime = tf.clip_by_value(xt + tf.sign(grad_x)/256,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:    \n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    a, b = sess.run([x_prime ,cost], feed_dict={x: xt, y: yt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.savetxt('output_grad.txt', a[0,:,:], delimiter=',', fmt='%.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sub = xt-a[0,:,:]\n",
    "# np.savetxt('output_diff.txt', sub, delimiter=',', fmt='%.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
