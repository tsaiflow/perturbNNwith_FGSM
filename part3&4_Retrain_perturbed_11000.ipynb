{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new1 = np.load('./checkpoints/x_newtrain1.npy')\n",
    "x_new5 = np.load('./checkpoints/x_newtrain5.npy')\n",
    "x_new10 = np.load('./checkpoints/x_newtrain10.npy')\n",
    "x_new20 = np.load('./checkpoints/x_newtrain20.npy')\n",
    "x_new30 = np.load('./checkpoints/x_newtrain30.npy')\n",
    "x_new40 = np.load('./checkpoints/x_newtrain40.npy')\n",
    "x_new50 = np.load('./checkpoints/x_newtrain50.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 20\n",
    "batch_size = 100\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition => 10 classes\n",
    "\n",
    "# Set model weights\n",
    "W1 = tf.Variable(tf.random_normal([784, 300], mean=0, stddev=1))\n",
    "b1 = tf.Variable(tf.random_normal([300], mean=0, stddev = 1))\n",
    "\n",
    "#W2 = tf.Variable(tf.random_normal([300, 100], mean=0, stddev= 1))\n",
    "#b2 = tf.Variable(tf.random_normal([100], mean=0, stddev= 1))\n",
    "\n",
    "W3 = tf.Variable(tf.zeros([300, 10]))\n",
    "b3 = tf.Variable(tf.zeros([10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct model\n",
    "\n",
    "hidden1 = tf.nn.relu(tf.matmul(x, W1) + b1); #first hidden layer\n",
    "\n",
    "#hidden2 = tf.nn.relu(tf.matmul(hidden1, W2) + b2); #second hidden layer\n",
    "\n",
    "pred = tf.nn.softmax(tf.matmul(hidden1, W3) + b3) # Softmax layer outputs prediction probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Minimize error using cross entropy \n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimizer = {}\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# for name, fcn in cost:\n",
    "#     optimizer[\"optimizer_{0}\".format(name)] = tf.train.GradientDescentOptimizer(learning_rate).minimize(fcn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_comb1 = np.append(mnist.train.images, x_new1, axis = 0)\n",
    "x_comb5 = np.append(mnist.train.images, x_new5, axis = 0)\n",
    "x_comb10 = np.append(mnist.train.images, x_new10, axis = 0)\n",
    "x_comb20 = np.append(mnist.train.images, x_new20, axis = 0)\n",
    "x_comb30 = np.append(mnist.train.images, x_new30, axis = 0)\n",
    "x_comb40 = np.append(mnist.train.images, x_new40, axis = 0)\n",
    "x_comb50 = np.append(mnist.train.images, x_new50, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(mnist.train.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110000, 784)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_comb1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# y_comb = np.append(mnist.train.labels, mnist.test.labels[:3000], axis = 0)\n",
    "\n",
    "y_comb = np.append(mnist.train.labels, mnist.train.labels, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = data[ idx]\n",
    "    labels_shuffle = labels[ idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n",
    "\n",
    "def attack_success_rate(xts, xts_new, yts):\n",
    "        # Result of old test data\n",
    "        prediction_old = tf.argmax(pred,1)\n",
    "        prediction_old = prediction_old.eval({x: xts})    \n",
    "        \n",
    "        correct_prediction = tf.equal(prediction_old, tf.argmax(yts, 1))\n",
    "        correct_prediction = correct_prediction.eval({x: xts})\n",
    "        \n",
    "        # Because we are only \n",
    "        correct_prediction_index = np.where(correct_prediction)\n",
    "        \n",
    "        xts_correct = xts_new[correct_prediction_index,:]\n",
    "        xts_correct = xts_correct[0,:,:]\n",
    "        \n",
    "        correct_prediction = correct_prediction[correct_prediction_index]\n",
    "        prediction_old = prediction_old[correct_prediction_index]\n",
    "        \n",
    "        # Result of new test data\n",
    "        prediction_new = tf.argmax(pred,1)\n",
    "        prediction_new = prediction_new.eval({x:xts_correct})\n",
    "        \n",
    "        #changed_prediction = tf.not_equal(prediction_old, prediction_new)\n",
    "        #changed_prediction = changed_prediction.eval({x: xts_correct})\n",
    "        \n",
    "        # Vind out which index of correct_predictions are changed after perturb\n",
    "        attack_success_index = np.not_equal(prediction_old, prediction_new)\n",
    "        \n",
    "        # Calculaye attack ratio\n",
    "        attack_success_no = np.count_nonzero(attack_success_index)\n",
    "        correct_prediction_no = np.count_nonzero(correct_prediction)\n",
    "        \n",
    "        attack_success_rate = attack_success_no/correct_prediction_no\n",
    "        \n",
    "        return attack_success_rate\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "# save_dir = 'checkpoints/'\n",
    "# if not os.path.exists(save_dir):\n",
    "#     os.makedirs(save_dir)\n",
    "# save_path = os.path.join(save_dir, 'trained_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # coding: utf-8\n",
    "\n",
    "# # Start training\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "#     # Training cycle\n",
    "#     for epoch in range(training_epochs):\n",
    "#         avg_cost = 0.\n",
    "#         total_batch = int(np.shape(x_comb1)[0]/batch_size)\n",
    "# #         print(\"number of total_batch: %\"%s)\n",
    "#         # Loop over all batches\n",
    "#         for i in range(total_batch):\n",
    "            \n",
    "# #             batch_xs, batch_ys = next_batch(total_batch, x_comb1, y_comb)\n",
    "# #             batch_xs, batch_ys = next_batch(total_batch, x_comb5, y_comb)\n",
    "# #             batch_xs, batch_ys = next_batch(total_batch, x_comb10, y_comb)\n",
    "#             batch_xs, batch_ys = next_batch(total_batch, x_comb20, y_comb)\n",
    "# #             batch_xs, batch_ys = next_batch(total_batch, x_comb30, y_comb)\n",
    "# #             batch_xs, batch_ys = next_batch(total_batch, x_comb40, y_comb)\n",
    "# #             batch_xs, batch_ys = next_batch(total_batch, x_comb50, y_comb)\n",
    "            \n",
    "#             # Fit training using batch data\n",
    "#             _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,y: batch_ys})\n",
    "            \n",
    "# #             print(__w)\n",
    "            \n",
    "#             # Compute average loss\n",
    "#             avg_cost += c / total_batch\n",
    "#         # Display logs per epoch step\n",
    "#         if (epoch+1) % display_step == 0:\n",
    "# #             print(sess.run(W))\n",
    "#             print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "#     print (\"Optimization Finished!\")\n",
    "\n",
    "#     # Test model\n",
    "#     correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    \n",
    "#     # Calculate accuracy for 3000 examples; you should get roughly ~90% accuracy although it might vary from run to run\n",
    "#     accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#     print (\"Accuracy:\", accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]}))\n",
    "    \n",
    "    \n",
    "#     #save weights of the pertubed model\n",
    "# #     saver.save(sess, './checkpoints/trained_model_p1.ckpt')\n",
    "# #     saver.save(sess, './checkpoints/trained_model_p5.ckpt')\n",
    "# #     saver.save(sess, './checkpoints/trained_model_p10.ckpt')\n",
    "#     saver.save(sess, './checkpoints/trained_model_p20.ckpt')\n",
    "# #     saver.save(sess, './checkpoints/trained_model_p30.ckpt')\n",
    "# #     saver.save(sess, './checkpoints/trained_model_p40.ckpt')\n",
    "# #     saver.save(sess, './checkpoints/trained_model_p50.ckpt')\n",
    "\n",
    "#     print(\"Model saved\")\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model(no retraining).\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/trained_model.ckpt\n",
      "Accuracy of original test dataset(3000): 0.902333\n",
      "Accuracy of perturbed(eps = 1) train dataset: 0.897545\n",
      "Accuracy of perturbed(eps = 5) train dataset: 0.647691\n",
      "Accuracy of perturbed(eps = 10) train dataset: 0.207218\n",
      "Accuracy of perturbed(eps = 20) train dataset: 0.00298182\n",
      "Accuracy of perturbed(eps = 30) train dataset: 0.000127273\n",
      "Accuracy of perturbed(eps = 40) train dataset: 7.27273e-05\n",
      "Accuracy of perturbed(eps = 50) train dataset: 0.0\n",
      "Accuracy of mixed normal & perturbed FGSM dataset: 0.568655\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    print(\"Original model(no retraining).\")\n",
    "    saver.restore(sess, \"./checkpoints/trained_model.ckpt\")\n",
    "    \n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    \n",
    "    # Calculate accuracy for 3000 examples; you should get roughly ~90% accuracy although it might vary from run to run\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#     print (\"Accuracy of original train dataset:\", accuracy.eval({x: mnist.train.images, y: mnist.train.labels}))\n",
    "    print (\"Accuracy of original test dataset(3000):\", accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]}))\n",
    "    print (\"Accuracy of perturbed(eps = 1) train dataset:\", accuracy.eval({x: x_new1, y: mnist.train.labels}))\n",
    "    print (\"Accuracy of perturbed(eps = 5) train dataset:\", accuracy.eval({x: x_new5, y: mnist.train.labels}))\n",
    "    print (\"Accuracy of perturbed(eps = 10) train dataset:\", accuracy.eval({x: x_new10, y: mnist.train.labels}))\n",
    "    print (\"Accuracy of perturbed(eps = 20) train dataset:\", accuracy.eval({x: x_new20, y: mnist.train.labels}))\n",
    "    print (\"Accuracy of perturbed(eps = 30) train dataset:\", accuracy.eval({x: x_new30, y: mnist.train.labels}))\n",
    "    print (\"Accuracy of perturbed(eps = 40) train dataset:\", accuracy.eval({x: x_new40, y: mnist.train.labels}))\n",
    "    print (\"Accuracy of perturbed(eps = 50) train dataset:\", accuracy.eval({x: x_new50, y: mnist.train.labels}))\n",
    "    print (\"Accuracy of mixed normal & perturbed FGSM dataset:\", accuracy.eval({x: np.append(x_new10, mnist.train.images, axis = 0), \n",
    "                                       y: np.append(mnist.train.labels, mnist.train.labels, axis = 0)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrained eps = 1 model\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/trained_model_p1.ckpt\n",
      "Accuracy of original test dataset(3000): 0.909667\n",
      "Accuracy of perturbed(eps = 1) train dataset: 0.933873\n",
      "Accuracy of mixed normal & perturbed FGSM dataset: 0.934973\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    print(\"Retrained eps = 1 model\")\n",
    "    saver.restore(sess, \"./checkpoints/trained_model_p1.ckpt\")\n",
    "    \n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    \n",
    "    # Calculate accuracy for 3000 examples; you should get roughly ~90% accuracy although it might vary from run to run\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#     print (\"Accuracy of original train dataset:\", accuracy.eval({x: mnist.train.images, y: mnist.train.labels}))\n",
    "    print (\"Accuracy of original test dataset(3000):\", accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]}))\n",
    "    print (\"Accuracy of perturbed(eps = 1) train dataset:\", accuracy.eval({x: x_new1, y: mnist.train.labels}))\n",
    "    print (\"Accuracy of mixed normal & perturbed FGSM dataset:\", accuracy.eval({x: np.append(x_new1, mnist.train.images, axis = 0), \n",
    "                                       y: np.append(mnist.train.labels, mnist.train.labels, axis = 0)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrained eps = 5 model\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/trained_model_p5.ckpt\n",
      "Accuracy of original test dataset(3000): 0.912333\n",
      "Accuracy of perturbed(eps = 5) train dataset: 0.935291\n",
      "Accuracy of mixed normal & perturbed FGSM dataset: 0.935609\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    print(\"Retrained eps = 5 model\")\n",
    "    saver.restore(sess, \"./checkpoints/trained_model_p5.ckpt\")\n",
    "    \n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    \n",
    "    # Calculate accuracy for 3000 examples; you should get roughly ~90% accuracy although it might vary from run to run\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#     print (\"Accuracy of original train dataset:\", accuracy.eval({x: mnist.train.images, y: mnist.train.labels}))\n",
    "    print (\"Accuracy of original test dataset(3000):\", accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]}))\n",
    "    print (\"Accuracy of perturbed(eps = 5) train dataset:\", accuracy.eval({x: x_new5, y: mnist.train.labels}))\n",
    "    print (\"Accuracy of mixed normal & perturbed FGSM dataset:\", accuracy.eval({x: np.append(x_new5, mnist.train.images, axis = 0), \n",
    "                                       y: np.append(mnist.train.labels, mnist.train.labels, axis = 0)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrained eps = 10 model\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/trained_model_p10.ckpt\n",
      "Accuracy of original test dataset(3000): 0.905667\n",
      "Accuracy of perturbed(eps = 10) train dataset: 0.943164\n",
      "Accuracy of mixed normal & perturbed FGSM dataset: 0.935855\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    print(\"Retrained eps = 10 model\")\n",
    "    saver.restore(sess, \"./checkpoints/trained_model_p10.ckpt\")\n",
    "    \n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    \n",
    "    # Calculate accuracy for 3000 examples; you should get roughly ~90% accuracy although it might vary from run to run\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#     print (\"Accuracy of original train dataset:\", accuracy.eval({x: mnist.train.images, y: mnist.train.labels}))\n",
    "    print (\"Accuracy of original test dataset(3000):\", accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]}))\n",
    "    print (\"Accuracy of perturbed(eps = 10) train dataset:\", accuracy.eval({x: x_new10, y: mnist.train.labels}))\n",
    "    print (\"Accuracy of mixed normal & perturbed FGSM dataset:\", accuracy.eval({x: np.append(x_new10, mnist.train.images, axis = 0), \n",
    "                                       y: np.append(mnist.train.labels, mnist.train.labels, axis = 0)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrained eps = 20 model\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/trained_model_p20.ckpt\n",
      "Accuracy of original test dataset(3000): 0.890333\n",
      "Accuracy of perturbed(eps = 20) train dataset: 0.967855\n",
      "Accuracy of mixed normal & perturbed FGSM dataset: 0.943464\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    print(\"Retrained eps = 20 model\")\n",
    "    saver.restore(sess, \"./checkpoints/trained_model_p20.ckpt\")\n",
    "    \n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    \n",
    "    # Calculate accuracy for 3000 examples; you should get roughly ~90% accuracy although it might vary from run to run\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#     print (\"Accuracy of original train dataset:\", accuracy.eval({x: mnist.train.images, y: mnist.train.labels}))\n",
    "    print (\"Accuracy of original test dataset(3000):\", accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]}))\n",
    "    print (\"Accuracy of perturbed(eps = 20) train dataset:\", accuracy.eval({x: x_new20, y: mnist.train.labels}))\n",
    "    print (\"Accuracy of mixed normal & perturbed FGSM dataset:\", accuracy.eval({x: np.append(x_new20, mnist.train.images, axis = 0), \n",
    "                                       y: np.append(mnist.train.labels, mnist.train.labels, axis = 0)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrained eps = 30 model\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/trained_model_p30.ckpt\n",
      "Accuracy of original test dataset(3000): 0.884667\n",
      "Accuracy of perturbed(eps = 30) train dataset: 0.761982\n",
      "Accuracy of mixed normal & perturbed FGSM dataset: 0.838391\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    print(\"Retrained eps = 30 model\")\n",
    "    saver.restore(sess, \"./checkpoints/trained_model_p30.ckpt\")\n",
    "    \n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    \n",
    "    # Calculate accuracy for 3000 examples; you should get roughly ~90% accuracy although it might vary from run to run\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#     print (\"Accuracy of original train dataset:\", accuracy.eval({x: mnist.train.images, y: mnist.train.labels}))\n",
    "    print (\"Accuracy of original test dataset(3000):\", accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]}))\n",
    "    print (\"Accuracy of perturbed(eps = 30) train dataset:\", accuracy.eval({x: x_new30, y: mnist.train.labels}))\n",
    "    print (\"Accuracy of mixed normal & perturbed FGSM dataset:\", accuracy.eval({x: np.append(x_new30, mnist.train.images, axis = 0), \n",
    "                                       y: np.append(mnist.train.labels, mnist.train.labels, axis = 0)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrained eps = 40 model\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/trained_model_p40.ckpt\n",
      "Accuracy of original test dataset(3000): 0.886333\n",
      "Accuracy of perturbed(eps = 40) train dataset: 0.984073\n",
      "Accuracy of mixed normal & perturbed FGSM dataset: 0.948273\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    print(\"Retrained eps = 40 model\")\n",
    "    saver.restore(sess, \"./checkpoints/trained_model_p40.ckpt\")\n",
    "    \n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    \n",
    "    # Calculate accuracy for 3000 examples; you should get roughly ~90% accuracy although it might vary from run to run\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#     print (\"Accuracy of original train dataset:\", accuracy.eval({x: mnist.train.images, y: mnist.train.labels}))\n",
    "    print (\"Accuracy of original test dataset(3000):\", accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]}))\n",
    "    print (\"Accuracy of perturbed(eps = 40) train dataset:\", accuracy.eval({x: x_new40, y: mnist.train.labels}))\n",
    "    print (\"Accuracy of mixed normal & perturbed FGSM dataset:\", accuracy.eval({x: np.append(x_new40, mnist.train.images, axis = 0), \n",
    "                                       y: np.append(mnist.train.labels, mnist.train.labels, axis = 0)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrained eps = 50 model\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/trained_model_p50.ckpt\n",
      "Accuracy of original test dataset(3000): 0.889667\n",
      "Accuracy of perturbed(eps = 50) train dataset: 0.497127\n",
      "Accuracy of mixed normal & perturbed FGSM dataset: 0.7072\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    print(\"Retrained eps = 50 model\")\n",
    "    saver.restore(sess, \"./checkpoints/trained_model_p50.ckpt\")\n",
    "    \n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    \n",
    "    # Calculate accuracy for 3000 examples; you should get roughly ~90% accuracy although it might vary from run to run\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#     print (\"Accuracy of original train dataset:\", accuracy.eval({x: mnist.train.images, y: mnist.train.labels}))\n",
    "    print (\"Accuracy of original test dataset(3000):\", accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]}))\n",
    "    print (\"Accuracy of perturbed(eps = 50) train dataset:\", accuracy.eval({x: x_new50, y: mnist.train.labels}))\n",
    "    print (\"Accuracy of mixed normal & perturbed FGSM dataset:\", accuracy.eval({x: np.append(x_new50, mnist.train.images, axis = 0), \n",
    "                                       y: np.append(mnist.train.labels, mnist.train.labels, axis = 0)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# # saver = tf.train.Saver()\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#   # Restore variables from disk.\n",
    "#   saver.restore(sess, \"/tmp/model.ckpt\")\n",
    "#   print(\"Model restored.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
